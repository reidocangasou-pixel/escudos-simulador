# -*- coding: utf-8 -*-
"""Baixador de Escudos

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e5iGmThB1To36SCFZvBD0j7X7d0Flrqy
"""

import os
import requests
import unicodedata
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# --- CONFIGURAÇÃO ---
# URL da página que contém os escudos
SOURCE_URL = "https://www.escudosweb.com/brasileirao-serie-a/"
# Pasta onde os logos serão salvos
OUTPUT_DIR = "logos"
# Lista de times que queremos baixar. O nome deve ser o mesmo usado na página de origem.
TEAMS_TO_DOWNLOAD = [
    "Atlético-MG", "Bahia", "Botafogo", "Ceará", "Corinthians", "Cruzeiro",
    "Flamengo", "Fluminense", "Fortaleza", "Juventude", "Grêmio", "Internacional",
    "Mirassol", "Palmeiras", "RB Bragantino", "Santos", "São Paulo", "Sport",
    "Vasco", "Vitória"
]

def slugify(text):
    """
    Cria um 'slug' a partir de um texto, compatível com o que o JavaScript espera.
    Ex: "São Paulo" -> "sao-paulo"
    """
    text = text.lower()
    text = re.sub(r'\s+br\b', '', text)
    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')
    text = re.sub(r'[\s\.]+', '-', text)
    text = re.sub(r'[^a-z0-9\-]', '', text)
    text = re.sub(r'-+', '-', text)
    text = text.strip('-')
    return text

def download_logos():
    """
    Função principal que baixa e salva os escudos.
    """
    print(f"--- Iniciando download dos escudos de '{SOURCE_URL}' ---")

    # 1. Cria o diretório de saída se ele não existir
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"Diretório '{OUTPUT_DIR}' garantido.")

    # 2. Faz o request HTTP para obter o conteúdo da página
    try:
        response = requests.get(SOURCE_URL, headers={'User-Agent': 'Mozilla/5.0'})
        response.raise_for_status()  # Lança um erro se o status não for 200 (OK)
    except requests.exceptions.RequestException as e:
        print(f"\n[ERRO] Falha ao acessar a URL: {e}")
        return

    # 3. Parseia o HTML da página
    soup = BeautifulSoup(response.content, 'html.parser')

    # 4. Encontra todos os elementos que contêm os escudos.
    #    Analisando a página, cada escudo está dentro de um <a> com a classe 'link-img-escudo'
    logo_links = soup.find_all('a', class_='link-img-escudo')

    if not logo_links:
        print("\n[ERRO] Não foi possível encontrar os links dos escudos na página.")
        print("A estrutura do site pode ter mudado. Verifique a classe 'link-img-escudo'.")
        return

    found_teams = {}
    for link in logo_links:
        img_tag = link.find('img')
        if img_tag and 'alt' in img_tag.attrs:
            team_name = img_tag['alt'].replace('Escudo ', '').strip()
            img_url = urljoin(SOURCE_URL, img_tag['src'])
            found_teams[team_name] = img_url

    # 5. Itera sobre a nossa lista de times e baixa os que encontrar
    download_count = 0
    for team_name in TEAMS_TO_DOWNLOAD:
        if team_name in found_teams:
            img_url = found_teams[team_name]

            try:
                img_response = requests.get(img_url, stream=True, headers={'User-Agent': 'Mozilla/5.0'})
                img_response.raise_for_status()

                # Gera o nome do arquivo a partir do nome do time
                file_slug = slugify(team_name)
                file_path = os.path.join(OUTPUT_DIR, f"{file_slug}.png")

                # Salva a imagem
                with open(file_path, 'wb') as f:
                    for chunk in img_response.iter_content(chunk_size=8192):
                        f.write(chunk)

                print(f"[SUCESSO] Escudo do '{team_name}' salvo em '{file_path}'")
                download_count += 1

            except requests.exceptions.RequestException as e:
                print(f"[FALHA] Não foi possível baixar o escudo do '{team_name}'. Erro: {e}")
        else:
            print(f"[AVISO] Escudo para '{team_name}' não encontrado na página.")

    print(f"\n--- Processo finalizado: {download_count}/{len(TEAMS_TO_DOWNLOAD)} escudos baixados. ---")

if __name__ == "__main__":
    download_logos()